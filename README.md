# Alvessa: agentic AI scientist

## Installation
```bash
conda activate agents
pip install -e .
```

Export required API tokens:
```bash
export ANTHROPIC_API_KEY=...
export BioGRID_API_KEY=...
export DISGENET_API_KEY=...
```

### CLI usage
- `alvessa question "Summarize GO terms enriched for interactors of TP53"`
- `alvessa tools`
- `alvessa ui 9000` (serves the UI on `http://127.0.0.1:9000`; default is 8000)

## Quickstart commands
```bash
# install environment from requirements.txt
conda activate agents
pip install -r requirements.txt
export ANTHROPIC_API_KEY=…
export BioGRID_API_KEY=…
python -m run
```

Run the FastAPI UI:
```bash
python -m serve_ui
```

Execute unit tests:
```bash
pytest tests/test_entity_recognition.py
```

## Repository layout
- `run.py` – main CLI entry point orchestrating the LangGraph pipeline and saving outputs.
- `src/alvessa/workflow/graph_builder.py` – builds graph nodes, edges, and workflow diagram.
- `src/alvessa/clients/claude.py` – Claude client with rate limiting and retries.
- `src/config.py` – central configuration and environment variable accessors.
- `src/state.py` – shared LangGraph state structure definition.
- `src/alvessa/domain/` – classes for genes, variants, and related entities.
- `src/alvessa/agents/` – Claude-backed tool selector, verification, entity extraction, and summarization agents.
- `src/tools/<name>/node.py` – deterministic tool adapters and shared utilities.
- `web/` – UI assets (`ui.html`, `static/`, `images/`) consumed by `serve_ui.py`.
- `serve_ui.py` – FastAPI app that serves the dashboard and proxies pipeline runs.
- `evals/generation/` – benchmark dataset generation (GWAS, BioGRID, Reactome, Gencode, LabBench).
- `evals/benchmarks_general.py` – common harness used by benchmark runners.
- `evals/plots/plot_evaluations.py` – plotting utilities for benchmark CSVs.
- `tests/` – pytest suites with `test_entity_recognition.py` as the primary unit test.
- `local_dbs/` – reference datasets required by generation scripts and tools (read-only).
- `out/` – run outputs refreshed by `python -m run`.

## Artifacts & logs
- `demo.txt` – saved answers and evidence summaries.
- `demo.log` – pipeline log including tool call traces.
- `demo.json` – serialized pipeline state for UI consumption.
- `graph_diagram.png` – LangGraph visualization generated by the workflow builder.
